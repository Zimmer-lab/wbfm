import logging
import os
import warnings
from dataclasses import dataclass
from typing import List, Tuple, Optional

import numpy as np
import pandas as pd
import scipy
from matplotlib import pyplot as plt
from scipy.cluster import hierarchy
from tqdm.auto import tqdm

from wbfm.utils.external.utils_behavior_annotation import BehaviorCodes
from wbfm.utils.external.utils_pandas import get_contiguous_blocks_from_column, remove_short_state_changes
from wbfm.utils.external.utils_zeta_statistics import calculate_zeta_cumsum, jitter_indices, calculate_p_value_from_zeta
from wbfm.utils.general.utils_matplotlib import paired_boxplot_from_dataframes
from wbfm.utils.tracklets.high_performance_pandas import get_names_from_df
from wbfm.utils.visualization.filtering_traces import filter_gaussian_moving_average, fill_nan_in_dataframe
from wbfm.utils.visualization.utils_plot_traces import plot_with_shading


def plot_triggered_average_from_matrix_low_level(triggered_avg_matrix, ind_preceding, min_lines,
                                                 show_individual_lines, is_second_plot, ax, xlim=None, **kwargs):
    raw_trace_mean, triggered_avg, triggered_std, xmax, is_valid = \
        TriggeredAverageIndices.prep_triggered_average_for_plotting(triggered_avg_matrix, min_lines=min_lines)
    if not is_valid:
        logging.warning("Found invalid neuron (empty triggered average)")
        return None, None
    # Plot
    ax, lower_shading, upper_shading = plot_with_shading(triggered_avg, triggered_std, xmax, ax, **kwargs)
    if show_individual_lines:
        for trace in triggered_avg_matrix:
            ax.plot(trace[:xmax], 'black', alpha=5.0 / (triggered_avg_matrix.shape[0] + 10.0))
    if not is_second_plot:
        ax.set_ylabel("Activity")
        ax.set_ylim(np.nanmin(lower_shading), np.nanmax(upper_shading))
        # Reference points
        ax.axhline(raw_trace_mean, c='black', ls='--')
        ax.axvline(x=ind_preceding, color='r', ls='--')
    else:
        ax.autoscale()
    if xlim is not None:
        ax.set_xlim(xlim)
    return ax, triggered_avg


@dataclass
class TriggeredAverageIndices:
    """
    Class for keeping track of all the settings related to a general triggered average
    By default triggered average time points are calculated from the binarized behavioral annotation
        Optionally, a continuous variable can be used
        This is designed to work on a wrapped phase variable, so that a binary variable can be generated by thresholding

    Has all postprocessing functions, so that analysis is consistent when calculated for multiple traces

    The traces themselves are not stored here
    """
    # Initial calculation of indices
    behavioral_annotation: pd.Series
    behavioral_state: int = BehaviorCodes.REV  # Note: not used if behavioral_annotation_is_continuous is True
    min_duration: int = 0
    ind_preceding: int = 10

    max_duration: int = None
    gap_size_to_remove: int = None

    behavioral_annotation_is_continuous: bool = False
    behavioral_annotation_threshold: float = 0.0  # Not used if behavioral_annotation_is_continuous is False

    # Postprocessing the trace matrix (per trace)
    trace_len: int = None
    to_nan_points_of_state_before_point: bool = True
    min_lines: int = 2
    include_censored_data: bool = True  # To include events whose termination is after the end of the data
    dict_of_events_to_keep: dict = None
    mean_subtract: bool = False
    z_score: bool = False
    normalize_amplitude_at_onset: bool = False

    DEBUG: bool = False

    @property
    def binary_state(self):
        if self.behavioral_annotation_is_continuous:
            binary_state = self.behavioral_annotation > self.behavioral_annotation_threshold
        else:
            binary_state = self.behavioral_annotation == self.behavioral_state
        if self.gap_size_to_remove is not None:
            binary_state = remove_short_state_changes(binary_state, self.gap_size_to_remove)
        return binary_state

    @property
    def cleaned_binary_state(self):
        # I can make this cached, but then it has to be cleared if the binary_state changes
        if self.trace_len is not None:
            return self.binary_state.iloc[:self.trace_len]
        else:
            return self.binary_state

    def triggered_average_indices(self, dict_of_events_to_keep=None, DEBUG=False) -> list:
        """
        Calculates triggered average indices based on a binary state vector saved in this class

        If ind_preceding > 0, then a very early event will lead to negative indices
        Thus in later steps, the trace should be padded with nan at the end to avoid wrapping

        Parameters
        ----------
        dict_of_events_to_keep: Optional dict determining a subset of indices to keep. Key=state starts, value=0 or 1
            Example:
            all_starts = [15, 66, 114, 130]
            dict_of_ind_to_keep = {15: 0, 66: 1, 114: 0}

            Note that not all starts need to be in dict_of_ind_to_keep; missing entries are dropped by default

        Returns
        -------

        """
        if dict_of_events_to_keep is None:
            dict_of_events_to_keep = self.dict_of_events_to_keep
        else:
            self.dict_of_events_to_keep = dict_of_events_to_keep
        binary_state = self.cleaned_binary_state.copy()
        all_starts, all_ends = get_contiguous_blocks_from_column(binary_state,
                                                                 already_boolean=True, skip_boolean_check=True)

        if DEBUG:
            print("All starts: ", all_starts)
            print("All ends: ", all_ends)
        # Turn into time series
        beh_vec = self.behavioral_annotation.to_numpy()
        min_duration, max_duration, ind_preceding = self.min_duration, self.max_duration, self.ind_preceding
        # Build all validity checks as a list of callables
        is_too_short = lambda start, end: end - start < min_duration
        is_too_long = lambda start, end: (max_duration is not None) and (end - start > max_duration)
        is_at_edge = lambda start, end: start == 0
        starts_with_misannotation = lambda start, end: beh_vec[start-1] == BehaviorCodes.UNKNOWN
        not_in_dict = lambda start, end: (dict_of_events_to_keep is not None) and \
                                         (dict_of_events_to_keep.get(start, 0) == 0)
        validity_checks = [is_too_short, is_too_long, is_at_edge, starts_with_misannotation, not_in_dict]
        # Build actual indices
        all_ind = build_ind_matrix_from_starts_and_ends(all_ends, all_starts, ind_preceding, validity_checks, DEBUG)
        return all_ind

    def calc_triggered_average_matrix(self, trace, custom_ind: List[np.ndarray]=None,
                                      nan_times_with_too_few=False, max_len=None,
                                      **ind_kwargs) -> Optional[np.ndarray]:
        """
        Uses triggered_average_indices to extract a matrix of traces at each index, with nan padding to equalize the
        lengths of the traces

        If there are no valid indices, returns None

        Parameters
        ----------
        trace
        custom_ind: instead of using self.triggered_average_indices. If not None, ind_kwargs are not used
        nan_times_with_too_few
        max_len: Cut off matrix at a time point. Usually if there aren't enough data points that far
        ind_kwargs

        Returns
        -------

        """
        if custom_ind is None:
            all_ind = self.triggered_average_indices(**ind_kwargs)
        else:
            all_ind = custom_ind
        if len(all_ind) == 0:
            return None
        if max_len is None:
            max_len_subset = max(map(len, all_ind))
        else:
            max_len_subset = max_len
        # Pad with nan in case there are negative indices, but only the end
        trace = np.pad(trace, max_len_subset, mode='constant', constant_values=(np.nan, np.nan))[max_len_subset:]
        triggered_avg_matrix = np.zeros((len(all_ind), max_len_subset))
        triggered_avg_matrix[:] = np.nan
        # Save either entire traces, or traces up to a point
        for i, ind in enumerate(all_ind):
            if max_len is not None:
                ind = ind.copy()[:max_len]
            triggered_avg_matrix[i, np.arange(len(ind))] = trace[ind]

        # Postprocessing type 1: change amplitudes
        if self.mean_subtract:
            triggered_avg_matrix -= np.nanmean(triggered_avg_matrix, axis=1, keepdims=True)
        if self.z_score:
            triggered_avg_matrix /= np.nanstd(triggered_avg_matrix, axis=1, keepdims=True)
        if self.normalize_amplitude_at_onset:
            # Normalize to the amplitude at the index of the event
            triggered_avg_matrix = triggered_avg_matrix - triggered_avg_matrix[:, [self.ind_preceding]]

        # Postprocessing type 2: remove points
        if self.to_nan_points_of_state_before_point:
            triggered_avg_matrix = self.nan_points_of_state_before_point(triggered_avg_matrix, all_ind)
        if nan_times_with_too_few:
            num_lines_at_each_time = np.sum(~np.isnan(triggered_avg_matrix), axis=0)
            times_to_remove = num_lines_at_each_time < self.min_lines
            triggered_avg_matrix[:, times_to_remove] = np.nan

        return triggered_avg_matrix

    def calc_null_triggered_average_matrix(self, trace, **kwargs):
        """Similar to calc_triggered_average_matrix, but jitters the indices"""
        triggered_average_indices = self.triggered_average_indices()
        ind_jitter = jitter_indices(triggered_average_indices, max_jitter=len(trace), max_len=len(trace))
        mat_jitter = self.calc_triggered_average_matrix(trace, custom_ind=ind_jitter, **kwargs)
        return mat_jitter

    def nan_points_of_state_before_point(self, triggered_average_mat, list_of_triggered_ind):
        """
        Checks points up to a certain level, and nans them if they are invalid. Only checks up to a certain threshold

        Parameters
        ----------
        triggered_average_mat

        Returns
        -------

        """
        if self.behavioral_annotation_is_continuous:
            invalid_states = {True}
            beh_annotations = self.cleaned_binary_state
        else:
            invalid_states = {self.behavioral_state, BehaviorCodes.UNKNOWN}
            beh_annotations = self.behavioral_annotation.to_numpy()
        for i_trace in range(len(list_of_triggered_ind)):
            these_ind = list_of_triggered_ind[i_trace]
            for i_local, i_global in enumerate(these_ind):
                if i_global < 0:
                    continue
                if i_local >= self.ind_preceding:
                    break
                if beh_annotations[i_global] in invalid_states:
                    # Remove all points before this
                    for i_to_remove in range(i_local + 1):
                        triggered_average_mat[i_trace, i_to_remove] = np.nan
        return triggered_average_mat

    @staticmethod
    def prep_triggered_average_for_plotting(triggered_avg_matrix, min_lines, shorten_to_last_valid=True):
        triggered_avg, triggered_std, triggered_avg_counts = \
            TriggeredAverageIndices.calc_triggered_average_stats(triggered_avg_matrix)
        # Remove points where there are too few lines contributing
        to_remove = triggered_avg_counts < min_lines
        triggered_avg[to_remove] = np.nan
        triggered_std[to_remove] = np.nan
        xmax = pd.Series(triggered_avg).last_valid_index()
        if shorten_to_last_valid:
            # Helps with plotting individual lines, but will likely produce traces of different lengths
            triggered_avg = triggered_avg[:xmax]
            triggered_std = triggered_std[:xmax]
        raw_trace_mean = np.nanmean(triggered_avg)
        is_valid = len(triggered_avg) > 0 and np.count_nonzero(~np.isnan(triggered_avg)) > 0
        return raw_trace_mean, triggered_avg, triggered_std, xmax, is_valid

    @staticmethod
    def calc_triggered_average_stats(triggered_avg_matrix):
        triggered_avg = np.nanmean(triggered_avg_matrix, axis=0)
        triggered_std = np.nanstd(triggered_avg_matrix, axis=0)
        triggered_avg_counts = np.nansum(~np.isnan(triggered_avg_matrix), axis=0)
        return triggered_avg, triggered_std, triggered_avg_counts

    def calc_significant_points_from_triggered_matrix(self, triggered_avg_matrix):
        """
        Calculates the time points that are (based on the std) "significantly" different from a flat line

        Designed to be used to remove uninteresting traces from triggered average grid plots

        Parameters
        ----------
        triggered_avg_matrix

        Returns
        -------

        """
        raw_trace_mean, triggered_avg, triggered_std, xmax, is_valid = \
            self.prep_triggered_average_for_plotting(triggered_avg_matrix, self.min_lines)
        if not is_valid:
            return []
        upper_shading = triggered_avg + triggered_std
        lower_shading = triggered_avg - triggered_std
        x_significant = np.where(np.logical_or(lower_shading > raw_trace_mean, upper_shading < raw_trace_mean))[0]
        return x_significant

    def calc_p_value_using_zeta(self, trace, num_baseline_lines=100, DEBUG=False) -> float:
        """
        See utils_zeta_statistics. Following:
        https://elifesciences.org/articles/71969#

        Parameters
        ----------
        trace
        num_baseline_lines

        Returns
        -------

        """
        # Original triggered average matrix
        triggered_average_indices = self.triggered_average_indices()
        # Set max number of time points based on number of lines present
        # In other words, find the max point in time when there are still enough lines
        if self.min_lines > 0:
            all_lens = np.array(list(map(len, triggered_average_indices)))
            ind_lens_enough = np.argsort(all_lens)[:-self.min_lines]
            max_matrix_length = np.max(all_lens[ind_lens_enough])
        else:
            max_matrix_length = None
        mat = self.calc_triggered_average_matrix(trace, custom_ind=triggered_average_indices,
                                                 max_len=max_matrix_length)
        zeta_line_dat = calculate_zeta_cumsum(mat, DEBUG=DEBUG)

        if DEBUG:
            print(max_matrix_length)

            plt.figure(dpi=100)
            self.plot_triggered_average_from_matrix(mat, show_individual_lines=True)
            plt.title("Triggered average")

            plt.figure(dpi=100)
            plt.plot(np.sum(~np.isnan(mat), axis=0))
            plt.title("Number of lines contributing to each point")
            plt.show()

        # Null distribution
        if max_matrix_length is None:
            mat_len = mat.shape[1]
        else:
            mat_len = max_matrix_length
        baseline_lines = self.calc_null_distribution_of_triggered_lines(mat_len,
                                                                        num_baseline_lines, trace,
                                                                        triggered_average_indices)

        # if DEBUG:
        #     plt.figure(dpi=100)
        #     all_ind_jitter = np.hstack(all_ind_jitter)
        #     plt.hist(all_ind_jitter)
        #     plt.title("Number of times each data point is selected")
        #     plt.show()

        # Normalize by the std of the baseline
        # Note: calc the std across trials, then average across time
        baseline_per_line_std = np.std(baseline_lines, axis=0)
        baseline_std = np.mean(baseline_per_line_std)

        zeta_line_dat /= baseline_std
        baseline_lines /= baseline_std

        if DEBUG:
            plt.figure(dpi=100)
            plt.plot(zeta_line_dat)
            for i_row in range(baseline_lines.shape[0]):
                line = baseline_lines[i_row, :]
                plt.plot(line, 'gray', alpha=0.1)
            plt.ylabel("Deviation (std of baseline)")
            plt.title("Trace zeta line and null distribution")
            plt.show()

        # Calculate individual zeta values (max deviation)
        zeta_dat = np.max(np.abs(zeta_line_dat))
        zetas_baseline = np.max(np.abs(baseline_lines), axis=1)

        # ALT: calculate sum of squares, and plot
        # Idea: maybe I can do chi squared instead
        # Following: https://stats.stackexchange.com/questions/200886/what-is-the-distribution-of-sum-of-squared-errors
        # if DEBUG:
        #     zeta2_dat = np.sum(np.abs(zeta_line_dat)**2.0)
        #     zetas2_baseline = np.sum(np.abs(baseline_lines)**2.0, axis=1)
        #
        #     # What is the df for time series errors?
        #     p2 = 1 - scipy.stats.chi2.cdf(zeta2_dat, 2)
        #
        #     plt.figure(dpi=100)
        #     plt.hist(zetas2_baseline)#, bins=np.arange(0, np.max(zetas2_baseline)))
        #     plt.vlines(zeta2_dat, 0, len(zetas_baseline) / 2, colors='red')
        #     plt.title(f"Distribution of sum of squares, with p={p2}")
        #     plt.show()

        # Final p value
        p = calculate_p_value_from_zeta(zeta_dat, zetas_baseline)

        if DEBUG:
            plt.figure(dpi=100)
            plt.hist(zetas_baseline)
            plt.vlines(zeta_dat, 0, len(zetas_baseline) / 2, colors='red')
            plt.title(f"Distribution of maxima of null, with p value: {p}")
            plt.show()

        return p

    def calc_null_distribution_of_triggered_lines(self, max_matrix_length, num_baseline_lines, trace,
                                                  triggered_average_indices):
        baseline_lines = np.zeros((num_baseline_lines, max_matrix_length))
        all_ind_jitter = []
        for i in range(num_baseline_lines):
            ind_jitter = jitter_indices(triggered_average_indices, max_jitter=len(trace), max_len=len(trace))
            mat_jitter = self.calc_triggered_average_matrix(trace, custom_ind=ind_jitter,
                                                            max_len=max_matrix_length)
            zeta_line = calculate_zeta_cumsum(mat_jitter)
            baseline_lines[i, :] = zeta_line
            all_ind_jitter.extend(ind_jitter)
            # if DEBUG:
            #     time.sleep(2)
        return baseline_lines

    def calc_p_value_using_ttest(self, trace, gap=5, DEBUG=False) -> Tuple[float, float]:
        """
        Calculates a p value using a paired t-test on the pre- and post-stimulus time periods

        Note that this is generally sensitive to ind_preceding (in addition to other arguments0

        Parameters
        ----------
        trace
        num_baseline_lines

        Returns
        -------

        """
        mat = self.calc_triggered_average_matrix(trace)
        if mat is None:
            return 1, 0
        means_before, means_after = self.split_means_from_triggered_average_matrix(mat, gap=gap)
        p = scipy.stats.ttest_rel(means_before, means_after, nan_policy='omit').pvalue
        effect_size = np.nanmean(means_after) - np.nanmean(means_before)

        if DEBUG:
            self.plot_triggered_average_from_matrix(mat, show_individual_lines=True)
            plt.title(f"P value: {p}")

            df = pd.DataFrame([means_before, means_after]).dropna(axis=1)
            paired_boxplot_from_dataframes(df)
            plt.title(f"P value: {p}")

            plt.show()

        return p, effect_size

    def split_means_from_triggered_average_matrix(self, mat, gap):
        """Gets mean of trace before and after the trigger (same window length)"""
        i_trigger = self.ind_preceding
        num_pts = i_trigger - gap
        with warnings.catch_warnings():
            warnings.simplefilter(action='ignore', category=RuntimeWarning)
            means_before = np.nanmean(mat[:, 0:num_pts], axis=1)
            means_after = np.nanmean(mat[:, i_trigger:i_trigger + num_pts], axis=1)
        return means_before, means_after

    def plot_triggered_average_from_matrix(self, triggered_avg_matrix, ax=None,
                                           show_individual_lines=False,
                                           color_significant_times=False,
                                           is_second_plot=False,
                                           **kwargs):
        """
        Core plotting function; must be passed a matrix

        Parameters
        ----------
        triggered_avg_matrix
        ax
        show_individual_lines
        color_significant_times
        kwargs

        Returns
        -------

        """

        min_lines = self.min_lines
        ind_preceding = self.ind_preceding
        ax, triggered_avg = plot_triggered_average_from_matrix_low_level(triggered_avg_matrix, ind_preceding,
                                                                         min_lines, show_individual_lines,
                                                                         is_second_plot, ax, **kwargs)
        if ax is None:
            return
        # Optional orange points
        x_significant = self.calc_significant_points_from_triggered_matrix(triggered_avg_matrix)
        if color_significant_times:
            if len(x_significant) > 0:
                ax.plot(x_significant, triggered_avg[x_significant], 'o', color='tab:orange')

    def plot_ind_over_trace(self, trace):
        """
        Plots the indices stored here over a trace (for debugging)

        Parameters
        ----------
        trace

        Returns
        -------

        """

        plt.figure(dpi=100)
        plt.plot(trace)

        for ind in self.triggered_average_indices():
            ind = np.array(ind)
            ind = ind[ind > 0]
            plt.plot(ind, trace[ind], '.', color='tab:orange')
            plt.plot(ind[self.ind_preceding], trace[ind[self.ind_preceding]], 'o', color='tab:red')

    @property
    def idx_onsets(self):
        local_idx_of_onset = self.ind_preceding
        idx_onsets = np.array([vec[local_idx_of_onset] for vec in self.triggered_average_indices() if
                               vec[local_idx_of_onset] > 0])
        return idx_onsets

    def onset_vector(self):
        onset_vec = np.zeros(self.trace_len)
        onset_vec[self.idx_onsets] = 1
        return onset_vec

    @property
    def num_events(self):
        return len(self.idx_onsets)


@dataclass
class FullDatasetTriggeredAverages:
    """
    A class that uses TriggeredAverageIndices to process each trace of a full dataset (Dataframe) into a matrix of
    triggered averages

    Also has functions for plotting
    """
    df_traces: pd.DataFrame

    # Calculating indices
    ind_class: TriggeredAverageIndices

    # Calculating full average
    mean_subtract_each_trace: bool = False
    min_lines: int = 2
    min_points_for_significance: int = 5
    significance_calculation_method: str = 'ttest'  # Or: 'num_points'

    # Plotting
    show_individual_lines: bool = True
    color_significant_times: bool = True

    @property
    def neuron_names(self):
        names = list(set(self.df_traces.columns.get_level_values(0)))
        names.sort()
        return names

    def triggered_average_matrix_from_name(self, name):
        return self.ind_class.calc_triggered_average_matrix(self.df_traces[name])

    def df_of_all_triggered_averages(self):
        """
        Just saves the mean of the triggered average

        Fills nan by default
        """
        df_triggered = {}
        for name in self.neuron_names:
            mat = self.triggered_average_matrix_from_name(name)
            raw_trace_mean, triggered_avg, triggered_std, xmax, is_valid = \
                self.ind_class.prep_triggered_average_for_plotting(mat, min_lines=self.min_lines,
                                                                   shorten_to_last_valid=False)
            df_triggered[name] = triggered_avg

        df_triggered = pd.DataFrame(df_triggered)
        df_triggered = df_triggered.loc[:df_triggered.last_valid_index()]
        df_triggered = fill_nan_in_dataframe(df_triggered)
        return df_triggered

    def which_neurons_are_significant(self, min_points_for_significance=None, num_baseline_lines=100,
                                      ttest_gap=5, DEBUG=False):
        if min_points_for_significance is not None:
            self.min_points_for_significance = min_points_for_significance
        names_to_keep = []
        all_p_values = {}
        all_effect_sizes = {}
        for name in tqdm(self.neuron_names, leave=False):
            if DEBUG:
                print("======================================")
                print(name)

            if self.significance_calculation_method == 'zeta':
                logging.warning("Zeta calculation is unstable for calcium imaging!")
                trace = self.df_traces[name]
                p = self.ind_class.calc_p_value_using_zeta(trace, num_baseline_lines, DEBUG=DEBUG)
                all_p_values[name] = p
                to_keep = p < 0.05
            elif self.significance_calculation_method == 'num_points':
                logging.warning("Number of points calculation is not statistically justified!")
                mat = self.triggered_average_matrix_from_name(name)
                x_significant = self.ind_class.calc_significant_points_from_triggered_matrix(mat)
                all_p_values[name] = x_significant
                to_keep = len(x_significant) > self.min_points_for_significance
            elif self.significance_calculation_method == 'ttest':
                trace = self.df_traces[name]
                p, effect_size = self.ind_class.calc_p_value_using_ttest(trace, ttest_gap, DEBUG=DEBUG)
                all_p_values[name] = p
                all_effect_sizes[name] = effect_size
                to_keep = p < 0.05
            else:
                raise NotImplementedError(f"Unrecognized significance_calculation_method: "
                                          f"{self.significance_calculation_method}")

            if to_keep:
                names_to_keep.append(name)

        if len(names_to_keep) == 0:
            logging.warning("Found no significant neurons, subsequent steps may not work")

        return names_to_keep, all_p_values, all_effect_sizes

    def plot_single_neuron_triggered_average(self, neuron, ax=None):
        y = self.df_traces[neuron]
        self.ax_plot_func_for_grid_plot(None, y, ax, neuron)

    def ax_plot_func_for_grid_plot(self, t, y, ax, name, **kwargs):
        """Same as ax_plot_func_for_grid_plot, but can be used directly"""
        if kwargs.get('is_second_plot', False):
            # Do not want two legend labels
            if 'label' in kwargs:
                kwargs['label'] = ''
            plot_kwargs = dict(label='')
        else:
            plot_kwargs = dict(label=name)
        plot_kwargs.update(kwargs)

        mat = self.ind_class.calc_triggered_average_matrix(y)
        self.ind_class.plot_triggered_average_from_matrix(mat, ax, **plot_kwargs)
        # ax.axhline(0, c='black', ls='--')
        # ax.plot(self.ind_class.ind_preceding, 0, "r>", markersize=10)

    @staticmethod
    def load_from_project(project_data, trigger_opt=None, trace_opt=None, **kwargs):
        if trigger_opt is None:
            trigger_opt = {}
        if trace_opt is None:
            trace_opt = {}

        trigger_opt_default = dict(min_lines=3, ind_preceding=20)
        trigger_opt_default.update(trigger_opt)
        ind_class = project_data.worm_posture_class.calc_triggered_average_indices(**trigger_opt_default)

        trace_opt_default = dict(channel_mode='dr_over_r_20', calculation_mode='integration', min_nonnan=0.9)
        trace_opt_default.update(trace_opt)
        df_traces = project_data.calc_default_traces(**trace_opt_default)

        triggered_averages_class = FullDatasetTriggeredAverages(df_traces, ind_class, **kwargs)

        return triggered_averages_class


@dataclass
class ClusteredTriggeredAverages:

    df_triggered: pd.DataFrame
    df_corr: pd.DataFrame = None

    # For plotting individual clusters
    triggered_averages_class: FullDatasetTriggeredAverages = None
    linkage_threshold: float = 4.0  # TODO: better way to get clusters
    Z: np.ndarray = None
    clust_ind: np.ndarray = None

    verbose: int = 0

    def __post_init__(self):

        # Make sure all lists of names are aligned
        self.df_triggered = self.df_triggered.sort_values(by=0, axis='columns')

        # Calculate distance matrix (correlation, which is robust to nan)
        if self.verbose >= 1:
            print("Calculating correlation")
        df_corr = self.df_triggered.corr()
        self.df_corr = df_corr

        # Calculate clustering for further analysis
        if self.verbose >= 1:
            print("Calculating clustering")
        Z = hierarchy.linkage(df_corr.to_numpy(), method='complete', optimal_ordering=False)
        clust_ind = hierarchy.fcluster(Z, t=self.linkage_threshold, criterion='distance')
        names = self.names

        per_cluster_names = {}
        for i_clust in np.unique(clust_ind):
            per_cluster_names[i_clust] = names[clust_ind == i_clust]

        self.per_cluster_names = per_cluster_names
        self.Z = Z
        self.clust_ind = clust_ind
        if self.verbose >= 1:
            print("Finished initializing!")

    @property
    def names(self):
        # return pd.Series(get_names_from_df(self.df_corr, to_sort=False))
        return pd.Series(list(self.df_corr.columns))

    def plot_clustergram(self, output_folder=None):
        X = self.df_corr.to_numpy()

        dist_fun = lambda X, metric: X  # df_corr is already the distance (similarity)
        import dash_bio
        clustergram = dash_bio.Clustergram(X,
                                           dist_fun=dist_fun,
                                           row_labels=list(self.names),
                                           column_labels=list(self.names),
                                           height=800,
                                           width=800,
                                           color_threshold=
                                           {'row': self.linkage_threshold, 'col': self.linkage_threshold},
                                           center_values=False)
        if output_folder is not None:
            if not os.path.exists(output_folder):
                os.makedirs(output_folder, exist_ok=True)
            clustergram.write_image(os.path.join(output_folder, 'clustergram.png'))
        clustergram.show()

        return clustergram

    def plot_all_clusters(self):
        ind_class = self.triggered_averages_class.ind_class
        for i_clust, name_list in self.per_cluster_names.items():
            fig, ax = plt.subplots(dpi=200)
            # Build a pseudo-triggered average matrix, made of the means of each neuron
            pseudo_mat = []
            for name in name_list:
                triggered_avg = self.df_triggered[name].copy()
                pseudo_mat.append(triggered_avg)
            # Normalize the traces to be similar to the correlation, i.e. z-score them
            pseudo_mat = np.stack(pseudo_mat)
            pseudo_mat = pseudo_mat - np.nanmean(pseudo_mat, axis=1, keepdims=True)
            pseudo_mat = pseudo_mat / np.nanstd(pseudo_mat, axis=1, keepdims=True)
            # Plot
            ind_class.plot_triggered_average_from_matrix(pseudo_mat, ax, show_individual_lines=True)
            plt.title(f"Cluster {i_clust}/{len(self.per_cluster_names)} with {pseudo_mat.shape[0]} traces")

    def plot_all_clusters_simple(self, min_lines=0, ind_preceding=20, xlim=None, z_score=False,
                                 output_folder=None):
        """Like plot_all_clusters, but doesn't require a triggered_averages_class to be saved"""
        for i_clust, name_list in self.per_cluster_names.items():
            name_list = list(name_list)
            fig, ax = plt.subplots(dpi=200)
            # Build a pseudo-triggered average matrix, made of the means of each neuron
            pseudo_mat = []
            for name in name_list:
                triggered_avg = self.df_triggered[name].copy()
                pseudo_mat.append(triggered_avg)
            # Normalize the traces to be similar to the correlation, i.e. z-score them
            pseudo_mat = np.stack(pseudo_mat)
            if z_score:
                pseudo_mat = pseudo_mat - np.nanmean(pseudo_mat, axis=1, keepdims=True)
                pseudo_mat = pseudo_mat / np.nanstd(pseudo_mat, axis=1, keepdims=True)
            # Plot
            if np.isnan(pseudo_mat).all():
                continue
            # these_corr = self.df_corr.loc[name_list[0], name_list[1:]]
            # avg_corr = these_corr.mean()
            plot_triggered_average_from_matrix_low_level(pseudo_mat, ind_preceding, min_lines,
                                                         show_individual_lines=True, is_second_plot=False, ax=ax,
                                                         xlim=xlim)
            plt.title(f"Cluster {i_clust}/{len(self.per_cluster_names)} with {pseudo_mat.shape[0]} traces")
            if output_folder is not None:
                if not os.path.exists(output_folder):
                    os.makedirs(output_folder, exist_ok=True)
                plt.savefig(os.path.join(output_folder, f"cluster_{i_clust}.png"))
            plt.show()

    @staticmethod
    def load_from_project(project_data, trigger_opt=None):
        if trigger_opt is None:
            trigger_opt = {}
        # default_trigger_opt = dict(min_duration=10, state=BehaviorCodes.REV, ind_preceding=30)
        default_trigger_opt = {}
        default_trigger_opt.update(trigger_opt)
        triggered_averages_class = FullDatasetTriggeredAverages.load_from_project(project_data,
                                                                                  trigger_opt=default_trigger_opt)

        # Strongly filter to clean up the correlation matrix
        df = triggered_averages_class.df_traces.copy()
        triggered_averages_class.df_traces = filter_gaussian_moving_average(df, std=3)

        return ClusteredTriggeredAverages.load_from_triggered_average_class(triggered_averages_class)

    @staticmethod
    def load_from_triggered_average_class(triggered_averages_class):
        df_triggered = triggered_averages_class.df_of_all_triggered_averages()
        return ClusteredTriggeredAverages(df_triggered, triggered_averages_class=triggered_averages_class)


def ax_plot_func_for_grid_plot(t, y, ax, name, project_data, state, min_lines=4, **kwargs):
    """
    Designed to be used with make_grid_plot_using_project with the arg ax_plot_func=ax_plot_func
    Note that you must create a closure to remove the following args, and pass a lambda:
        project_data
        state
        min_lines

    Example:
    from functools import partial
    func = partial(ax_plot_func_for_grid_plot, project_data=p, state=1)

    Parameters
    ----------
    state: the state whose onset is calculated as the trigger
    min_lines: the minimum number of lines that must exist for a line to be plotted
    project_data
    t: time vector (unused)
    y: full trace (1d)
    ax: matplotlib axis
    name: neuron name
    kwargs

    Returns
    -------

    """
    plot_kwargs = dict(label=name)
    plot_kwargs.update(kwargs)

    ind_preceding = 20
    worm_class = project_data.worm_posture_class

    ind_class = worm_class.calc_triggered_average_indices(state=state, ind_preceding=ind_preceding,
                                                          min_lines=min_lines)
    mat = ind_class.calc_triggered_average_matrix(y)
    ind_class.plot_triggered_average_from_matrix(mat, ax, **plot_kwargs)


# def plot_triggered_average_from_matrix_with_histogram(triggered_avg_matrix, show_individual_lines=True):
#     triggered_avg, triggered_std, triggered_avg_counts = calc_triggered_average_stats(triggered_avg_matrix)
#
#     fig, axes = plt.subplots(nrows=2, sharex=True, dpi=100)
#
#     ax = axes[0]
#     plot_triggered_average_from_matrix(triggered_avg_matrix, ax, show_individual_lines)
#
#     triggered_avg_counts = np.nansum(~np.isnan(triggered_avg_matrix), axis=0)
#     x = np.arange(len(triggered_avg))
#     axes[1].bar(x, triggered_avg_counts)
#     axes[1].set_ylabel("Num contributing")
#     axes[1].set_xlabel("Time (frames)")
#
#     return axes


def assign_id_based_on_closest_onset_in_split_lists(class1_onsets, class0_onsets, rev_onsets) -> dict:
    """
    Assigns each reversal a class based on which list contains an event closes to that reversal

    Note if a reversal has no previous forward, it will be removed!

    Parameters
    ----------
    class1_onsets
    class0_onsets
    rev_onsets

    Returns
    -------

    """
    raise ValueError("Not working! See test")
    dict_of_rev_with_id = {}
    for rev in rev_onsets:
        # For both forward lists, get the previous indices
        these_class0 = class0_onsets.copy() - rev
        these_class0 = these_class0[these_class0 < 0]

        these_class1 = class1_onsets.copy() - rev
        these_class1 = these_class1[these_class1 < 0]

        # Then the smaller absolute one (closer in time) one gives the class
        only_prev_short = len(these_class0) == 0 and len(these_class1) > 0
        only_prev_long = len(these_class1) == 0 and len(these_class0) > 0
        # Do not immediately calculate, because the list may be empty
        short_is_closer = lambda: np.min(np.abs(these_class0)) < np.min(np.abs(these_class1))
        if only_prev_short:
            dict_of_rev_with_id[rev] = 0
        elif only_prev_long:
            dict_of_rev_with_id[rev] = 1
        elif short_is_closer():
            # Need to check the above two conditions before trying to evaluate this
            dict_of_rev_with_id[rev] = 0
        else:
            dict_of_rev_with_id[rev] = 1

        # Optimization: Finally, remove the used one from the fwd onset list

    return dict_of_rev_with_id


def build_ind_matrix_from_starts_and_ends(all_ends: List[int], all_starts: List[int], ind_preceding: int,
                                          validity_checks=None,
                                          DEBUG=False):
    if validity_checks is None:
        validity_checks = []
    all_ind = []
    for start, end in zip(all_starts, all_ends):
        if DEBUG:
            print("Checking block: ", start, end)
        # Check validity
        validity_vec = [check(start, end) for check in validity_checks]
        if any(validity_vec):
            if DEBUG:
                print("Skipping because: ", validity_vec)
            continue
        elif DEBUG:
            print("***Keeping***")
        ind = np.arange(start - ind_preceding, end)
        all_ind.append(ind)
    return all_ind


def calc_time_series_from_starts_and_ends(all_starts, all_ends, num_pts, min_duration=0, only_onset=False):
    state_trace = np.zeros(num_pts)
    for start, end in zip(all_starts, all_ends):
        if end - start < min_duration:
            continue

        if not only_onset:
            state_trace[start:end] = 1
        else:
            state_trace[start] = 1
    return state_trace

